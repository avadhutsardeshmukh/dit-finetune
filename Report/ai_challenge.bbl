\begin{thebibliography}{1}

\bibitem{bitfit}
Elad Ben~Zaken, Yoav Goldberg, and Shauli Ravfogel.
\newblock {B}it{F}it: Simple parameter-efficient fine-tuning for
  transformer-based masked language-models.
\newblock In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors,
  {\em Proceedings of the 60th Annual Meeting of the Association for
  Computational Linguistics (Volume 2: Short Papers)}, pages 1--9, Dublin,
  Ireland, May 2022. Association for Computational Linguistics.

\bibitem{vpt}
Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath
  Hariharan, and Ser-Nam Lim.
\newblock Visual prompt tuning.
\newblock In {\em European Conference on Computer Vision (ECCV)}, 2022.

\bibitem{lora}
Edward~J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean
  Wang, Lu~Wang, and Weizhu Chen.
\newblock Lo{RA}: Low-rank adaptation of large language models.
\newblock In {\em International Conference on Learning Representations}, 2022.

\bibitem{driveditfit}
Jiahang Tu, Wei Ji, Hanbin Zhao, Chao Zhang, Roger Zimmermann, and Hui Qian.
\newblock Driveditfit: Fine-tuning diffusion transformers for autonomous
  driving data generation.
\newblock {\em ACM Trans. Multimedia Comput. Commun. Appl.}, 21(3), March 2025.

\bibitem{difffit}
Enze Xie, Lewei Yao, Han Shi, Zhili Liu, Daquan Zhou, Zhaoqiang Liu, Jiawei Li,
  and Zhenguo Li.
\newblock Difffit: Unlocking transferability of large diffusion models via
  simple parameter-efficient fine-tuning.
\newblock In {\em 2023 IEEE/CVF International Conference on Computer Vision
  (ICCV)}, pages 4207--4216, 2023.

\bibitem{dreambooth}
Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and
  Kfir Aberman.
\newblock Dreambooth: Fine tuning text-to-image diffusion models for
  subject-driven generation.
\newblock 2022.

\bibitem{textualinversion}
Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or~Patashnik, Amit~H. Bermano, Gal
  Chechik, and Daniel Cohen-Or.
\newblock An image is worth one word: Personalizing text-to-image generation
  using textual inversion, 2022.

\bibitem{controlnet}
Lvmin Zhang, Anyi Rao, and Maneesh Agrawala.
\newblock Adding conditional control to text-to-image diffusion models.

\end{thebibliography}
