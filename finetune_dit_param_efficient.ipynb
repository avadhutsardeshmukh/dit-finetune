{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3zKCGeGrGMUb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import argparse\n",
        "import tqdm\n",
        "import torch, torchvision\n",
        "import torch.nn.functional as F\n",
        "from diffusers import DDIMScheduler, DiTTransformer2DModel, AutoencoderKL\n",
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms, datasets\n",
        "import os\n",
        "from torchvision.datasets import ImageFolder\n",
        "from typing import Any, Callable, cast, Optional, Union\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "IMG_EXTENSIONS = (\".jpg\", \".jpeg\", \".png\", \".ppm\", \".bmp\", \".pgm\", \".tif\", \".tiff\", \".webp\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSqbD0hwPQ0Z",
        "outputId": "0672e4b9-e264-4b91-b288-4a2a34b05a29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n",
            "DS-MVTec\n",
            "hazelnut_best_samples_119Epoch_param_eff.png\n",
            "samples_step_18_epoch_109.png\n",
            "samples_step_36_epoch_19.png\n",
            "samples_step_36_epoch_9.png\n",
            "screw_best_samples_29epoch_fullfinetune.png\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "#!unzip -q \"/content/drive/MyDrive/DS_MVTec.zip\" -d \"/content/drive/MyDrive/DefectSpectrum\"\n",
        "!ls /content/drive/MyDrive/DefectSpectrum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WfcsMMGwG8je",
        "outputId": "1a7d139c-0b17-4a10-b01f-be19b5ed03da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "def show_images(im_batch):\n",
        "    #Unnormalize from [-1, 1] to [0,1]\n",
        "    im_batch = im_batch*0.5 + 0.5\n",
        "    grid = torchvision.utils.make_grid(im_batch)\n",
        "    grid = grid.detach().cpu().permute(1,2,0) * 255\n",
        "    grid_im = Image.fromarray(np.array(grid).astype(np.uint8))\n",
        "    return grid_im\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def has_file_allowed_extension(filename: str, extensions):\n",
        "    \"\"\"Checks if a file is an allowed extension. Helper function for the dataset loader\n",
        "    Args:\n",
        "        filename (string): path to a file\n",
        "        extensions (tuple of strings): extensions to consider (lowercase)\n",
        "    Returns:\n",
        "        bool: True if the filename ends with one of given extensions\n",
        "    \"\"\"\n",
        "    return filename.lower().endswith(extensions if isinstance(extensions, str) else tuple(extensions))\n",
        "\n",
        "class ImageFolderSelectSubset(ImageFolder):\n",
        "    \"\"\"A custom data loader.\n",
        "    It selects specified number of samples from each class.\n",
        "    If a class does not have those samples then all available samples are selected.\n",
        "    Args:\n",
        "        max_samples_per_class (int) : Maximum number of samples to be collected per class\n",
        "        For each class, the number of samples will be min(available_samples, max_samples_per_class)\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        root,\n",
        "        extensions = IMG_EXTENSIONS,\n",
        "        transform = None,\n",
        "        target_transform = None,\n",
        "        is_valid_file = None,\n",
        "        allow_empty = False,\n",
        "        max_samples_per_class = None,\n",
        "    ) -> None:\n",
        "        super().__init__(root, transform=transform, target_transform=target_transform)\n",
        "        self.root = root\n",
        "        self.transform = transform\n",
        "        classes, class_to_idx = self.find_classes(self.root)\n",
        "        #\n",
        "        self.max_samples_per_class = max_samples_per_class\n",
        "        samples = self.make_dataset(\n",
        "            self.root,\n",
        "            class_to_idx=class_to_idx,\n",
        "            extensions=extensions,\n",
        "            is_valid_file=is_valid_file,\n",
        "            allow_empty=allow_empty,\n",
        "            max_samples_per_class = self.max_samples_per_class\n",
        "        )\n",
        "        self.extensions = extensions\n",
        "        self.classes = classes\n",
        "        self.class_to_idx = class_to_idx\n",
        "        self.samples = samples\n",
        "        self.targets = [s[1] for s in samples]\n",
        "\n",
        "    @staticmethod\n",
        "    def make_dataset(\n",
        "    directory,\n",
        "    class_to_idx = None,\n",
        "    extensions = None,\n",
        "    is_valid_file = None,\n",
        "    allow_empty = False,\n",
        "    max_samples_per_class = None,\n",
        "    ) :\n",
        "        \"\"\"Generates a list of samples of a form (path_to_sample, class).\n",
        "        See :class:`DatasetFolder` for details.\n",
        "        We override this method to select only max_samples_per_class samples for each class.\n",
        "        Returns : list[tuple[str, int]]\n",
        "        \"\"\"\n",
        "        directory = os.path.expanduser(directory)\n",
        "\n",
        "        if class_to_idx is None:\n",
        "            _, class_to_idx = self.find_classes(directory)\n",
        "        elif not class_to_idx:\n",
        "            raise ValueError(\"'class_to_index' must have at least one entry to collect any samples.\")\n",
        "\n",
        "        both_none = extensions is None and is_valid_file is None\n",
        "        both_something = extensions is not None and is_valid_file is not None\n",
        "        if both_none or both_something:\n",
        "            raise ValueError(\"Both extensions and is_valid_file cannot be None or not None at the same time\")\n",
        "\n",
        "        if extensions is not None:\n",
        "\n",
        "            def is_valid_file(x: str) -> bool:\n",
        "                return has_file_allowed_extension(x, extensions)  # type: ignore[arg-type]\n",
        "\n",
        "        is_valid_file = cast(Callable[[str], bool], is_valid_file)\n",
        "\n",
        "        instances = []\n",
        "        available_classes = set()\n",
        "        #samples_per_class = 1\n",
        "        for target_class in sorted(class_to_idx.keys()):\n",
        "            class_index = class_to_idx[target_class]\n",
        "            target_dir = os.path.join(directory, target_class)\n",
        "            if not os.path.isdir(target_dir):\n",
        "                continue\n",
        "            #Keeps track of the number of samples (file_paths) collected for current class.\n",
        "            samples_per_class = 0\n",
        "            for root, _, fnames in sorted(os.walk(target_dir, followlinks=True)):\n",
        "                for fname in sorted(fnames):\n",
        "                    path = os.path.join(root, fname)\n",
        "                    if is_valid_file(path):\n",
        "                        item = path, class_index\n",
        "                        instances.append(item)\n",
        "\n",
        "                        if target_class not in available_classes:\n",
        "                            available_classes.add(target_class)\n",
        "                        ###Check if max_samples has been collected for this class.\n",
        "                        samples_per_class +=1\n",
        "                        if (max_samples_per_class is not None\n",
        "                        and samples_per_class >= max_samples_per_class):\n",
        "                            #If yes, then stop collecting samples. Move to next class.\n",
        "                            break\n",
        "\n",
        "        empty_classes = set(class_to_idx.keys()) - available_classes\n",
        "        if empty_classes and not allow_empty:\n",
        "            msg = f\"Found no valid file for the classes {', '.join(sorted(empty_classes))}. \"\n",
        "            if extensions is not None:\n",
        "                msg += f\"Supported extensions are: {extensions if isinstance(extensions, str) else ', '.join(extensions)}\"\n",
        "            raise FileNotFoundError(msg)\n",
        "\n",
        "        return instances\n"
      ],
      "metadata": {
        "id": "0OOsvPv9mRJ1"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "UC0kMzU0HMa3"
      },
      "outputs": [],
      "source": [
        "def dataloader(image_size, batch_size, part_type, max_samples_per_type):\n",
        "    \"\"\" Data loader for defectspectrum\n",
        "    Args :\n",
        "        image_size (int) : The input image size\n",
        "        batch_size (int) : Batch size\n",
        "        part_type (str) : The product or part type from DefectSpectrum\n",
        "        max_samples_per_type : Maximum no of images to be loaded per defect type of the part\n",
        "    Retruns :\n",
        "        dataset (torchvision.datasets.ImageFolder or ImageFolderSelectSubset) : The dataset object\n",
        "        dataloader (torch.utils.DataLoader) : A dataloader for the dataset\n",
        "     \"\"\"\n",
        "    preprocess = transforms.Compose(\n",
        "        [\n",
        "            #transforms.CenterCrop(800),\n",
        "            transforms.Resize((image_size,image_size)),\n",
        "            #transforms.ColorJitter(brightness=(0.5,1.5),contrast=(3),saturation=(0.3,1.5),hue=(-0.1,0.1)),\n",
        "            #transforms.RandomRotation([45,120], interpolation=transforms.InterpolationMode.BILINEAR),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomVerticalFlip(),\n",
        "            #transforms.RandomEqualize(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5]), # From [0,1] to [-1,1] Normalization\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    #Direct access from hf\n",
        "    #dataset = load_dataset(\"huggan/smithsonian_butterflies_subset\", split=\"train\")\n",
        "            #DefectSpectrum/Defect_Spectrum\")\n",
        "    #In case the dataset is locally stored\n",
        "    dataset = ImageFolderSelectSubset(\n",
        "            os.path.join('/content/drive/MyDrive/DefectSpectrum/DS-MVTec',\n",
        "            part_type, 'image'),\n",
        "            transform=preprocess,\n",
        "            max_samples_per_class = max_samples_per_type\n",
        "            )\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "    return dataset, dataloader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "mcNFwQ9-HUVi"
      },
      "outputs": [],
      "source": [
        "def sample_diffusion(diffusion_model, vae_model, vae_latent_size, noise_scheduler, num_classes, num_samples_per_class):\n",
        "    \"\"\"\n",
        "    Sample from a (trained) latent diffusion model.\n",
        "    :param diffusion_model : The pretrained latent diffusion model\n",
        "    :param vae_model : The pretrained VAE model to map images to latent space\n",
        "    :param vae_latent_size : Size of the VAE latent - also the input size of diffusion model\n",
        "    :param noise_scheduler : The noise scheduler used by the diffusion model\n",
        "    :param num_classes : Number of different class labels for conditional generation\n",
        "    :param num_samples_per_class : The no of samples to be generated for each class\n",
        "\n",
        "    :return A grid image (PIL) of the generated samples\n",
        "    \"\"\"\n",
        "    labels = [l for l in range(num_classes) for _ in range(num_samples_per_class)]\n",
        "    labels = torch.tensor(labels, device = device).long()\n",
        "    labels = labels + 783  #for Imagenet adjustment\n",
        "    #################################################\n",
        "    \"\"\"\n",
        "    Closest class to \"screw\" from Imagent - 783\n",
        "    Closest class to \"hazelnut\" - 990\n",
        "    \"\"\"\n",
        "    #################################################\n",
        "    z = torch.randn(\n",
        "            num_samples_per_class,\n",
        "            vae_model.config.latent_channels,\n",
        "            vae_latent_size,\n",
        "            vae_latent_size,\n",
        "            device=device\n",
        "            )\n",
        "    #Replicate the noise vectors for all class labels\n",
        "    z = z.repeat(num_classes, 1,1,1)\n",
        "\n",
        "    #Sampling loop with the DDIMSampler\n",
        "    for i, t in enumerate(noise_scheduler.timesteps):\n",
        "        model_input = noise_scheduler.scale_model_input(z,t)\n",
        "        timestep = t[None].to(device)\n",
        "        #timestep = timestep.long()\n",
        "        with torch.no_grad():\n",
        "            #with torch.autocast(device_type='cpu'):\n",
        "            noise_pred = diffusion_model(model_input,timestep,labels,return_dict=False)[0]\n",
        "        mean_noise_pred = noise_pred[:,:4,:,:]\n",
        "        z = noise_scheduler.step(mean_noise_pred, t, z).prev_sample\n",
        "        #print('One dnoising step completed')\n",
        "\n",
        "    #print('Sampling done. Got latents of shape {}'.format(z.shape))\n",
        "    #Make a grid with num_samples_per_class images in each row, one row per class\n",
        "    #print(vae_model.config.scaling_factor)\n",
        "    x = vae_model.decode(z/vae_model.config.scaling_factor).sample\n",
        "    #print('Finally decoded input of shape ', x.shape)\n",
        "    grid = torchvision.utils.make_grid(x,nrow=num_classes)\n",
        "    im = grid.permute(1,2,0).cpu().clip(-1,1)*0.5 + 0.5\n",
        "    im = Image.fromarray(np.array(im*255).astype(np.uint8))\n",
        "    return im\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "2sSUeCmSOZTP"
      },
      "outputs": [],
      "source": [
        "def mark_only_biases_as_trainable(model: torch.nn.Module, is_bitfit=False):\n",
        "    if is_bitfit:\n",
        "        #Original BitFit only tunes biases. We also tune norm layers and label embedding\n",
        "        trainable_names = ['bias', 'norm', 'y_embed']\n",
        "    else:\n",
        "        trainable_names = [\"bias\",\"norm\",\"gamma\",\"y_embed\"]\n",
        "\n",
        "    for par_name, par_tensor in model.named_parameters():\n",
        "        par_tensor.requires_grad = any([kw in par_name for kw in trainable_names])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "eUMVdb5PHcyI"
      },
      "outputs": [],
      "source": [
        "def finetune(\n",
        "        full_fine_tune=False,\n",
        "        image_size=256,\n",
        "        #noise_scheduler_model = 'google/ddpm-celebahq-256',\n",
        "        part_type='hazelnut',\n",
        "        max_img_per_defect=None,\n",
        "        num_epochs=1,\n",
        "        lr=1e-5,\n",
        "        batch_size=4,\n",
        "        grad_acc_steps=2,\n",
        "        wandb_project='defectDiT_finetune',\n",
        "        ckpt_every=100,\n",
        "        ckpt_dir='/content/drive/MyDrive/DefectSpectrum',\n",
        "        log_samples_every=10,\n",
        "        patience=5\n",
        "        ):\n",
        "    \"\"\"\n",
        "    Fine tune a latent diffusion DiT model with a defect images of a particular part type from DefectSpectrum data.\n",
        "    :param full_fine_tune : To do full fine tuning if True, otherwise parameter efficient fine tuning (default False)\n",
        "    :param image_size : The input image size (default 256x256)\n",
        "    :param part_type : The selected part type from DefectSpectrum (e.g, zipper, pill,.. Default hazelnut)\n",
        "    :param max_img_per_defect : Maximum no of images to be loaded per defect type of the part (default None)\n",
        "    :param num_epochs : Number of epochs (default 1)\n",
        "    :param lr : Learning rate (default 1e-5, we don't use LR decay)\n",
        "    :param batch_size : Batch size (default = 4 to make it memory efficient)\n",
        "    :param grad_acc_steps : Number steps for which gradient is accumulated before updating wts - to account for small batch\n",
        "    :param wandb_project : The id of wandb project where intermediate models and outputs are logged (not used currently)\n",
        "    :param ckpt_every : Frequency of checkpointing the model (no of epochs)\n",
        "    :param ckpt_dir : currently the models are saved locally, so this parameter specifies the directory\n",
        "    :param log_samples_every : Frequency of saving generating samples (no of epochs)\n",
        "\n",
        "    :return A list of average loss values per epoch\n",
        "    \"\"\"\n",
        "\n",
        "    #Initialize the wandb project to log the samples and checkpoints during training\n",
        "    #TODO Enable this if it works in colab, otherwise save locally\n",
        "    #wandb.init(wandb_project, config=locals())\n",
        "\n",
        "    #Define the dataloader\n",
        "    db, dl = dataloader(image_size=image_size,batch_size=batch_size,part_type=part_type,max_samples_per_type=max_img_per_defect)\n",
        "    classes = db.classes\n",
        "    num_classes = len(db.classes)\n",
        "    #total_num_classes = 1000 + num_classes #Add the new classes to the 1000 from ImageNet\n",
        "    print(\"Prepared the dataloader with size\", len(dl))\n",
        "\n",
        "    #A fast scheduler to trade-off fidelity with sampling speed\n",
        "    scheduler=DDIMScheduler.from_pretrained('google/ddpm-celebahq-256')\n",
        "    scheduler.set_timesteps(num_inference_steps=200)\n",
        "\n",
        "    #The base DiT model. This is a latent diffusion model - i.e. it operates in the latent space of a VAE\n",
        "    # We add our new class labels to the 1000 of ImageNet, changing the attribute num_embeds_ada_norm\n",
        "    diffusion_model = DiTTransformer2DModel.from_pretrained(\n",
        "            'facebook/DiT-XL-2-256', subfolder='transformer',\n",
        "            #num_embeds_ada_norm=num_classes,\n",
        "            dropout = 0.2,\n",
        "            #low_cpu_mem_usage=False, ignore_mismatched_sizes=True\n",
        "            ).to(device)\n",
        "    #Parameter efficient tuning - only tune biases, norm and label embeddings\n",
        "    diffusion_model.train()\n",
        "    if (not full_fine_tune):\n",
        "      diffusion_model = mark_only_biases_as_trainable(diffusion_model, is_bitfit=True)\n",
        "\n",
        "    #Check if the trainable params are set correctly\n",
        "    #print(\"Non-Trainable Params\")\n",
        "    #for name, param in diffusion_model.named_parameters():\n",
        "        #if param.requires_grad:\n",
        "            #print(name)\n",
        "    # The VAE\n",
        "    vae_model = AutoencoderKL.from_pretrained(\n",
        "            'facebook/DiT-XL-2-256', subfolder='vae').to(device)\n",
        "    #TODO How to get this value from the model (is it vae.config.norm_num_groups)?\n",
        "    vae_latent_size=image_size//8\n",
        "\n",
        "\n",
        "    #Use a very small learning rate, since we have a very small dataset and a small batch size (gradients may be noisy)\n",
        "    optimizer = torch.optim.AdamW(diffusion_model.parameters(),lr=lr, weight_decay=0.0)\n",
        "    #Loss history, for posterior analysis or debugging\n",
        "    losses=[]\n",
        "\n",
        "    #Fine tuning loop\n",
        "    min_loss_epoch = 0\n",
        "    min_loss_till_now = 1e6\n",
        "    for epoch in range(num_epochs):\n",
        "        print(\"Training epoch\", epoch)\n",
        "        for step, batch in tqdm.tqdm(enumerate(dl), total=len(dl)):\n",
        "            train_images = batch[0].to(device)\n",
        "            train_labels = batch[1].to(device)\n",
        "            train_labels = train_labels + 783\n",
        "            \"\"\"Closest Imagent classes to our chosen product classes\n",
        "            Cloasest class for \"screw\" - 783\n",
        "            Closest class to \"hazelnut\" - 990\n",
        "\n",
        "            \"\"\"\n",
        "            #One hot\n",
        "            #train_labels = F.one_hot(train_labels, num_classes=total_num_classes)\n",
        "\n",
        "            #Get the latent representations of images from the VAE model - Diffusion will operate in this space\n",
        "            #We are not finetuning the VAE model (its generic, learns reps for any image). so, no_grad()\n",
        "            with torch.no_grad():\n",
        "                image_latents = vae_model.encode(train_images).latent_dist.sample()\n",
        "                image_latents = image_latents * vae_model.config.scaling_factor\n",
        "            #print(\"Got vae latents with shape \", image_latents.shape)\n",
        "\n",
        "            #Standard Gaussian noise to be added to each clean image\n",
        "            noise = torch.randn(image_latents.shape).to(device)\n",
        "            #Sample a timestep t uniformly for each real image in the batch\n",
        "            timesteps = torch.randint(0, scheduler.config.num_train_timesteps,\n",
        "                                      (image_latents.shape[0],),\n",
        "                                      device=image_latents.device).long()\n",
        "            #Add the noise, scaled with appropriate variance for that timestep (according to scheduler), to corresponding images\n",
        "            noisy_latents = scheduler.add_noise(image_latents, noise, timesteps)\n",
        "\n",
        "            #Predict the added noise from the VAE latent vectors of clean images\n",
        "            #The DiT model outputs two tensors - the predicted noise  and the diagonal covariance matrix\n",
        "            #Both of shape (patchXpatchXchannel), stacked together. We separate the mean predicted noise\n",
        "            with torch.autocast(device_type='cuda'):\n",
        "                ##Mixed precision training for memory efficiency\n",
        "                noise_pred = diffusion_model(\n",
        "                    noisy_latents,\n",
        "                    timesteps,train_labels,\n",
        "                    return_dict=False)\n",
        "                noise_pred = noise_pred[0]\n",
        "                #DiT output is of shape pX2Cxwxh - one C for mean noise and the other for std\n",
        "                mean_noise_pred = noise_pred[:,:4,:,:]\n",
        "                #Gradient descent on the error between the true added noise and predicted noise\n",
        "                loss = F.mse_loss(mean_noise_pred, noise)\n",
        "\n",
        "            losses.append(loss.item())\n",
        "            loss.backward(loss)\n",
        "\n",
        "            #Accumulate gradients for some steps - because small batch size might mean very small gradients\n",
        "            if (step % (grad_acc_steps) == 0):\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "        #Save some sample generations every 'log_samples_every' epochs\n",
        "        #Generate a batch of 8 images per class and save as a grid\n",
        "        if (epoch+1)%log_samples_every == 0:\n",
        "            num_samples_per_class=2\n",
        "            diffusion_model.eval()\n",
        "            im = sample_diffusion(\n",
        "                    diffusion_model, vae_model,\n",
        "                    vae_latent_size, scheduler,\n",
        "                    num_classes, num_samples_per_class\n",
        "                    )\n",
        "            diffusion_model.train()\n",
        "            save_path = f\"{ckpt_dir}/samples_step_{step}_epoch_{epoch}.png\"\n",
        "            im.save(save_path)\n",
        "            #wandb.log({'Sample generations': wandb.Image(im)})\n",
        "        # Save a checkpoint every 'ckpt_every' epochs\n",
        "        if (epoch+1)%ckpt_every == 0:\n",
        "              checkpoint_path=f\"{ckpt_dir}/checkpoint_step_{step+1}\"\n",
        "              checkpoint = {\n",
        "                        \"model\" : diffusion_model.state_dict(),\n",
        "                        \"optimizer\" : optimizer.state_dict()\n",
        "                      }\n",
        "              torch.save(checkpoint, checkpoint_path)\n",
        "        avg_loss = sum(losses[-len(dl):])/len(dl)\n",
        "        print(f\"Epoch {epoch}, Average Loss {avg_loss}\")\n",
        "        \"\"\"\n",
        "        #Maintain the epoch number with min avg loss. And if the loss doesnt improve for\n",
        "        #''patience'' number epochs, checkpoint the best model till now, save samples and quit\n",
        "        if(avg_loss < min_loss_till_now):\n",
        "          print(\"New Best \")\n",
        "          min_loss_loss_till_now = avg_loss\n",
        "          min_loss_epoch = epoch\n",
        "          num_samples_per_class=2\n",
        "          im = sample_diffusion(\n",
        "                  diffusion_model, vae_model,\n",
        "                  vae_latent_size, scheduler,\n",
        "                  num_classes, num_samples_per_class\n",
        "                  )\n",
        "          save_path = f\"{ckpt_dir}/best_samples_epoch_{epoch}.png\"\n",
        "          im.save(save_path)\n",
        "        if (min_loss_epoch < epoch - patience):\n",
        "          break\n",
        "          \"\"\"\n",
        "    return losses\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "OLQ9dypZHs9P",
        "outputId": "25530bda-0317-4aaf-fbf5-290e75f3fa74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prepared the dataloader with size 80\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "An error occurred while trying to fetch facebook/DiT-XL-2-256: facebook/DiT-XL-2-256 does not appear to have a file named diffusion_pytorch_model.safetensors.\n",
            "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n",
            "An error occurred while trying to fetch facebook/DiT-XL-2-256: facebook/DiT-XL-2-256 does not appear to have a file named diffusion_pytorch_model.safetensors.\n",
            "Defaulting to unsafe serialization. Pass `allow_pickle=False` to raise an error instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/80 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 9.38 MiB is free. Process 4652 has 22.15 GiB memory in use. Of the allocated memory 21.71 GiB is allocated by PyTorch, and 206.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-8869f9544634>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"PYTORCH_CUDA_ALLOC_CONF\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"expandable_segments:True\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m losses = finetune(\n\u001b[0m\u001b[1;32m      4\u001b[0m                 \u001b[0mfull_fine_tune\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                 \u001b[0mimage_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-05da47d6c498>\u001b[0m in \u001b[0;36mfinetune\u001b[0;34m(full_fine_tune, image_size, part_type, max_img_per_defect, num_epochs, lr, batch_size, grad_acc_steps, wandb_project, ckpt_every, ckpt_dir, log_samples_every, patience)\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;31m#Accumulate gradients for some steps - because small batch size might mean very small gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgrad_acc_steps\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m                             )\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"differentiable\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"betas\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             has_complex = self._init_group(\n\u001b[0m\u001b[1;32m    233\u001b[0m                 \u001b[0mgroup\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36m_init_group\u001b[0;34m(self, group, params_with_grad, grads, amsgrad, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps)\u001b[0m\n\u001b[1;32m    173\u001b[0m                 )\n\u001b[1;32m    174\u001b[0m                 \u001b[0;31m# Exponential moving average of squared gradient values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m                 state[\"exp_avg_sq\"] = torch.zeros_like(\n\u001b[0m\u001b[1;32m    176\u001b[0m                     \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreserve_format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                 )\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacity of 22.16 GiB of which 9.38 MiB is free. Process 4652 has 22.15 GiB memory in use. Of the allocated memory 21.71 GiB is allocated by PyTorch, and 206.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(42)\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "losses = finetune(\n",
        "                full_fine_tune=False,\n",
        "                image_size=128,\n",
        "                part_type='screw',\n",
        "                max_img_per_defect=50,\n",
        "                num_epochs=100,\n",
        "                lr=1e-5,\n",
        "                batch_size=2,\n",
        "                grad_acc_steps=2,\n",
        "                ckpt_every=100,\n",
        "                log_samples_every=10)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}